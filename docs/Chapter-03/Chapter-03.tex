\documentclass[10pt, oneside]{article}
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}

\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}


\title{MATE 5150: Elementary Matrix Operations and Systrems of Linear Equations}
\author{Alejandro Ouslan}
\date{Academic Year 2024-2025}

\begin{document}

\maketitle
\tableofcontents

\vspace{.25in}

\section{Elementary Matrix Operations and Elementary Matrices}

In this section, we define the elementary operations that are used throughout the chapter. In subsequent sections, we use these
operations to obtain simple computational methods for determining the rank of a linear transformation and the solution of a
system of linear equations. There are two types of elementary operations -row operations and column operations. As we will see,
the row operations are more useful. They arise from the three operations that can be used to eliminate variables in a system of linear equations.

\subsection{Elementary Operations}
\begin{defn}
	Let $A$ be an $m \times n$ matrix. Any of the following three operations on the rows [columns] of $A$ is called an \textbf{elementary row [column] operation}:
	\begin{enumerate}
		\item interchanging any two rows [columns] of $A$;
		\item multiplying any row [column] of $A$ by a nonzero scalar;
		\item adding any scalar multiple of a row [column] of $A$ to another row [column] of $A$.
	\end{enumerate}
\end{defn}

Any of these three operations is called an \textbf{elementary operation}. Elementary operations are of \textbf{type 1}, \textbf{type 2}, or \textbf{type 3} depending on
weather they are obtained by (1), (2), or (3) above.

\begin{defn}
	An $n \times n$ \textbf{elementary matrix} is a matrix obtained by performing an elementary operations on $I_n$. The elementary matrix is said to be of \textbf{type 1}, \textbf{type 2}, or \textbf{type 3}
	according to weather the elementary operation performed on $I_n$ is of type 1, type 2, or type 3.
\end{defn}

\subsubsection{Example Elementary Matrices}
Let
\[
	A = \begin{bmatrix} 1 & 2 & 3 \\ 1 & 1 & 1 \\ 1 & -1 & 1 \end{bmatrix} \quad ,
	B = \begin{bmatrix} 1 & 0 & 3 \\ 1 & -2 & 1 \\ 1 & -3 & 1 \end{bmatrix} \quad \text{and} \quad ,
	C = \begin{bmatrix} 1 & 0 & 0 \\ 0 & -2 & -2 \\ 1 & -3 & 1 \end{bmatrix}
\]

Find an elementary operation that transforms $A$ into $B$ and an elementary operation that transforms $B$ into $C$. By means of several additional operations, transform C into $I_3$.
\[
	\begin{split}
		AE = B \quad & \text{where} \quad E = \begin{bmatrix} 1 & -2 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \\
		EB = C \quad & \text{where} \quad E = \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \\
		E_1E_2E_3E_4 = I_3 \quad & \text{where} \quad E_1 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & 0 & 1 \end{bmatrix} \quad ,
		\quad E_2 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & -\frac{1}{2} & 0 \\ 0 & 0 & 1 \end{bmatrix} \quad ,
		\quad E_3 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 3 & 1 \end{bmatrix} \quad , E_4 = \begin{bmatrix} 1 & 0 & 3 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix}
	\end{split}
\]

\subsection{Properties of Elementary Matrices}
\begin{thm}
	Elementary matrices are invertible, and the inverse of an elementary matrix is an elementary matrix of the same type.
\end{thm}

\begin{proof}
	\begin{align*}
		\intertext{Let $E$ be an elementary matrix $n \times n$. The $E$ is defided by an elementary operation on $I_n$.}
	\end{align*}
\end{proof}

\subsubsection{Example}
Use the proof in Theorem 1 to obtain the inverse of each of the following elementary matrices:
\[
	A = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}, \quad
	B = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 1 \end{bmatrix}, \quad
	C = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -2 & 0 & 1 \end{bmatrix}
\]
Finding the inverse of each of the elementary matrices in the example above, we have:
\[
	\begin{split}
		E &= \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \end{bmatrix} \quad , \text{Therefore} \quad A^{-1} = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \end{bmatrix} \\
		E &= \begin{bmatrix} 1 & 0 & 0 \\ 0 & \frac{1}{3} & 0 \\ 0 & 0 & 1 \end{bmatrix} \quad , \text{Therefore} \quad B^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & \frac{1}{3} & 0 \\ 0 & 0 & 1 \end{bmatrix} \\
		E &= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 2 & 0 & 1 \end{bmatrix} \quad , \text{Therefore} \quad C^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -2 & 0 & 1 \end{bmatrix}
	\end{split}
\]

\subsection{Matrix Multiplication and Elementary Matrices}

Let $A$ be an $m \times n$ matrix. Prove that if $E$ can be obtained from $A$ by an elementary row [column] operation, then $B^T$ can be obtained from $A^T$ by the corresponding elementary column [row] operation.

\begin{proof}
	\begin{align*}
		(E_RB)^T & = (A)^T \\
		B^TE_R^T & = A^T
	\end{align*}
	Therefore, $B^T$ can be obtained from $A^T$ by the corresponding elementary column operation.
\end{proof}

\section{The Rank of a Matrix and Matrix Inverses}

In this section, we define the \textit{rank} of a matrix. We then use elementary operations to compute the rank of a matrix and a linear transformation. The section concludes with
a procedure for comping the inverse of an invertible matrix.

\begin{defn}
	If $A \in M_{m \times n(F)}$, We define the \textbf{rank} of $A$, denoted $rank(A)$, to be the rank of the linear transformation $L_A: F^n \to F^m$.
\end{defn}

Every matrix $A$ is the matrix representation of the linear transformation $L_A$ with reject to the appropriate standard ordure bases. These the rank of the linear transformation $L_A$
is the same as the rank of one of its matrix representations, namely, $A$. The next theorem extends this fact to any matrix representation of any linear transformation defined on finite-dimensional
vector spaces.

\begin{thm}
	Let $T:V \to W$ be a linear transformation between finite dimensional vector spaces, and let $\beta$ and $\gamma$ be ordered bases for $V$ and $W$, respectively.
	Then $rank(T) = rank([T]_{\beta}^{\gamma})$.
\end{thm}

\begin{thm}
	Let $A$ be an $m \times n$ matrix. If $P$ and $Q$ are invertible matrices of sizes $m \times m$ and $n \times n$, respectively, then
	\begin{enumerate}
		\item $rank(AQ) = rank(A)$
		\item $rank(PA) = rank(A)$
		\item $rank(PAQ) = rank(A)$
	\end{enumerate}
\end{thm}

\begin{proof}
	\begin{align*}
		R(L_{AQ}) & = R(L_A L_Q)    \\
		          & = L_A(L_Q(F^n)) \\
		          & = L_A(F^n)      \\
		          & = R(L_A)
	\end{align*}
	Since $L_A$ is onto, then $rank(AQ) = dim(R(L_{AQ})) = dim(R(L_A)) = rank(A)$
\end{proof}

\begin{cor}
	Elementary row and column operations on a matrix are rank-preserving.
\end{cor}

Now that we have a class of matrix operations that preserve rank, we need a way of examining a transformed matrix to ascertain its rank. The next theorem
is the first of several in this direction.

\begin{thm}
	The rank of any matrix equals the maximum number of its linearly independicen columns; that is the rank of a matrix is the dimension of the subspace generated by its columns.
\end{thm}

\subsubsection{Example}
Find the rank of the following matrix:
\[
	A = \begin{bmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1 & 1 & 0 \end{bmatrix}, \quad B = \begin{bmatrix} 1 & 1 & 0 \\ 2 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}
\]
Since $A$ can be represented as $a_2 = a_1 + a_3$, then $rank(A) = 2$. Since $B$ all of its columns are linearly independent, then $rank(B) = 3$.
\subsubsection{Example}
Prove that for any $m \times n$ matrix $A$, $rank(A) = 0$ if and only if $A$ is the zero matrix.

\begin{proof}
	\begin{align*}
		\intertext{The zero matrix has rank 0 because it has no linearly independent columns.}
		\intertext{Conversely, suppose that $rank(A) = 0$. Then the columns of $A$ are linearly dependent.}
	\end{align*}
	Therefore, $A$ has to be the zero matrix.
\end{proof}

The next theorem uses this process to transform a matrix into a particularly simple form. The power of this theorem can be seen in its corollaries.

\begin{thm}
	Let $A$ be an $m \times n$ matrix of rank $r$. Then $r \leq m$ , $r \leq n$, and by means of a finite number of elementary row and column operations, $A$ can be transformed into the matrix
	\[
		D = \begin{bmatrix} I_r & O_1 \\ O_2 & O_3 \end{bmatrix}
	\]
	Where $O_1$, $O_2$, and $O_3$ are zero matrices. Thus $D_{ii} = 1$ for $1 \leq i \leq r$ and $D_{ij} = 0$ otherwise.
\end{thm}

\begin{cor}
	Let $A$ be an $m \times n$ matrix of rank $r$. Then there exist invertible matrices $B$ and $C$ of sizes $m \times m$ and $n \times n$, respectively, such that $D = BAC$, where
	\[
		D = \begin{bmatrix} I_r & O_1 \\ O_2 & O_3 \end{bmatrix}
	\]
	is the $m \times n$ matrix in which $O_1$, $O_2$, and $O_3$ are zero matrices.
\end{cor}

\begin{cor}
	Let $A$ be an $m \times n$ matrix. Then
	\begin{enumerate}
		\item $rank(A^T) = rank(A)$.
		\item The rank of any matrix equals the maximum number of its linearly independent rows; that is, the rank if a matrix is the dimension of the subspace generated by its rows.
		\item The rows and columns of  any matrix generate subspace of the same dimensions, numerically equal to the rank of the matrix.
	\end{enumerate}
\end{cor}

\begin{cor}
	Every invertible matrix is a product of elementary matrices.
\end{cor}

\begin{proof}
	\begin{align*}
		\intertext{If $A$ is an invertible matrix, then $rank(A) = n$.}
		\intertext{Hence the matrix $D$ in corollary 2, $D = I_n$.}
		\intertext{Then was mutible matrix $B$ and $C$ such that $D = BAC$.}
		\intertext{By corollary 1, note that $B = E_pE_{p-1} \ldots E_1$ and $C = F_qF_{q-1} \ldots F_1$}
		\intertext{Therefore, $A = B^{-1}D(C^{-1})^{-1} = E_1E_2 \ldots E_pF_1F_2 \ldots F_q$}
	\end{align*}
	Therefore, every invertible matrix is a product of elementary matrices.
\end{proof}

\begin{thm}
	Let $T:V \to W$ and $U:W \to Z$ be linear transformations on finite-dimensional vector spaces $V$, $W$, and $Z$ and let $A$ and $B$ be matrices such that the product $AB$ is defined.
	Then
	\begin{enumerate}
		\item $rank(UT) \leq rank(U)$
		\item $rank(UT) \leq rank(T)$
		\item $rank(AB) \leq rank(A)$
		\item $rank(AB) \leq rank(B)$
	\end{enumerate}
\end{thm}

\begin{proof}
	\begin{align*}
		\intertext{Clearly, $R(T) \subseteq W$}
		R(UT) = U(T(V)) = U(R(T)) \subseteq U(W) = R(U)
	\end{align*}
	Therefore, $rank(UT) = dim(R(UT)) \leq dim(R(U)) = rank(U)$
\end{proof}

\subsubsection{Example}
Use elementary row and column operations to transform each of the following matrices into a matrix $D$ satisfying the conditions of Theorem 5, and then determine the rank of each matrix:
\[
	A = \begin{bmatrix} 1 & 1 & 1 & 2 \\ 2 & 0 & -1 & 2 \\ 1 & 1 & 1 & 2 \end{bmatrix} \quad , \quad B = \begin{bmatrix} 2 & 1 \\ -1 & 2 \\ 2 & 1 \end{bmatrix}
\]



\subsubsection{Example}
For each of the following linear transformation $T$, determine wheather $T$ is invertible, and compute $T^{-1}$ if it exists:

Let $T: \R^3 \to \R^3$ be defined by

\[
	T(a_1, a_2, a_3) = (a_1 + 2a_2 + a_3, -a_1 + a_2 + 2a_3, a_1 + a_3)
\]

\[
	\begin{split}
		[T]_{\beta} &= \begin{bmatrix} 1 & 2 & 1 \\ -1 & 1 & 2 \\ 1 & 0 & 1 \end{bmatrix} \\
		Rank([T]_{\beta}) &= 3 \\
		[T]_{\beta}^{-1} &= \begin{bmatrix} \frac{1}{6}  & -\frac{1}{3} & \frac{1}{2}  \\
                \frac{1}{2}  & 0            & -\frac{1}{2} \\
                -\frac{1}{6} & \frac{1}{3}  & \frac{1}{2}\end{bmatrix} \\
		T^{-1}(a_1, a_2, a_3) &= (\frac{1}{6}a_1 - \frac{1}{3}a_2 + \frac{1}{2}a_3, \frac{1}{2}a_1 - \frac{1}{2}a_3, -\frac{1}{6}a_1 + \frac{1}{3}a_2 + \frac{1}{2}a_3)
	\end{split}
\]

\subsection{The inverse of a Matrix}
We have remarked that an $n \times n$ matrix is invertible if and only if its rank is $n$. Since we know how to compute the rank of any matrix, we can always test a matrix
to termine weather its is invertible, We now provide a simple technique for computing the inverse of a matrix that utilizes elementary row operations.

\begin{thm}
	Let $A$ and $B$ be $m \times n$ and $m \times p$ matrices, respectively. By the \textbf{augmented matrix} $[A|B]$, we mean the $m \times (n + p)$ matrix, that is, the matrix whose first $n$ columns are
	the columns of $A$, and whose last $p$ columns are the columns of $B$.
\end{thm}

Conversely, suppose that $A$ is invertible and that, for some $m \times p$ matrix $B$, the matrix $[A|I_n]$ can be transformed into the matrix $[I_n|B]$ by a finite number of elementary row operations.
Let $E_1, E_2, \ldots, E_p$ be the elementary matrices associated with these elementary row operations as in Theorem 1. Then


\subsubsection{Example}
Express the invertible matrix
\[
	A = \begin{bmatrix} 1 & 2 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 2 \end{bmatrix}
\]
As a product of elementary matrices.

\[
	\begin{split}
		A = A E_1 E_2 E_3  \\
		A^{-1} = E_3 E_2 E_1
	\end{split}
\]

\section{Systems of Linear Equations - Theoretical Aspects}

\subsection{Systems of Linear Equations}
The system of equations
\[
	\begin{array}{ll}
		a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n & = b_1 \\
		a_{21}x_1 + a_{22}x_2 + \ldots + a_{2n}x_n & = b_2 \\
		\multicolumn{2}{c}{\vdots}                         \\
		a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n & = b_m
	\end{array}
\]
Where $a_{ij}$ and $b_i$ $(1 \leq i \leq m, 1 \leq j \leq n)$ are scalars in a field $F$, and $x_1, x_2, \ldots, x_n$ are $n$ variables taking values in $F$, is called a
\textbf{system of $m$ linear equations in $n$ unknowns over the field $F$.} The matrix
\[
	A = \begin{bmatrix} a_{11} & a_{12} & \ldots & a_{1n} \\ a_{21} & a_{22} & \ldots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \ldots & a_{mn} \end{bmatrix}
\]
Is called the \textbf{coefficient matrix} of the system, and the matrix. If we let
\[
	X = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \quad \text{and} \quad B = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{bmatrix}
\]
then the system $S$ may be rewritten as a single matrix equation $AX = B$. To exploit the results that we have developed, we often consider a system of linear equations as a
single matrix equation. A solution to the system $S$ is an $n$-tuple
\[
	\begin{bmatrix} s_1 \\ s_2 \\ \vdots \\ s_n \end{bmatrix} \in F^n
\]
such that $AS = B$. The set of all solutions to the system $S$ is called the \textbf{solution set} of the system. System $S$ is called \textbf{consistent} if its solution set is nonempty; otherwise
it is called inconsistent.

\subsection{Homogeneous Systems of Linear Equations}

We begin our sturcdy of systems of linear equations by examining the class of \textbf{homogeneous systems} of linear equations. Our first result shows that the set of solutions to a homogeneours system of $B$ linear
equations in $n$ unknows form a subspacce of $F^n$. We can then apply the theory of vector spaces to this set of solutions. For example, a basis for the solution space can be found, and any solution can be expressed as
a linear compination of the vector in the basis.

\begin{defn}
	A system $Ax = b$ of $b$ linear equations in $b$ unknowns is said to be \textbf{homogeneous} if $b = 0$. Otherwise, the system is said to be \textbf{nonhomogeneous}.
\end{defn}

Any homogeneous system has at least one solution, namely the zero vector. The result gives further information babout the set of solutions to a homogeneours system.

\begin{thm}
	Let $Ax = 0$ be a homogeneous system of $m$ linear equations in $n$ unknowns over a field $F$. Let $K$ denote the set of all solutions to $Ax = 0$. Then $K = Nul(L_A)$, hence $K$ is a subspace of $F^n$
	of dimension $n - rank(A)$.
\end{thm}

\begin{proof}
	\begin{align*}
		K = \{s \in F^n | As = 0\} = Nul(L_A)
	\end{align*}
\end{proof}

\begin{cor}
	If $m < n$, the system $Ax = 0$ has a nontrivial solution.
\end{cor}

\subsubsection{Example}

For each of the following homogeneous systems of linear equations, find the dimensions of and a basis for the solution space.
\[
	\begin{array}{ll}
		 & x_1 + 2x_2 - x_3 = 0 \\
		 & 2x_1 + x_2 + x_3 = 0
	\end{array}
\]

\[
	\begin{split}
		A = \begin{bmatrix} 1 & 2 & -1 \\ 2 & 1 & 1 \end{bmatrix} \quad , \quad rank(A) = 2 \\
		Nul(L_A) = \{s \in \R^3 | As = 0\} = \{s \in \R^3 | s = x_3 \begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix}\} \\
		\text{Therefore, the dimension of the solution space is 1 and a basis for the solution space is} \{ \begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix} \}
	\end{split}
\]

\begin{thm}
	Let $K$ be the solution set of a system of linear equations $Ax = b$, and let $K_H$ be the solution set of the corresponding homogeneous system $Ax = 0$. Then for any solution $s$ to $Ax = b$:
	$$ K = s + K_H = \{ s + h | h \in K_H \} $$
\end{thm}

\begin{proof}
	\begin{align*}
		\intertext{Suppose that $w \in \{s\} + K_H$}
		\intertext{Then $w = s + h$ for some $h \in K_H$}
		\intertext{Since $Aw = A(s + h) = As + Ak =  b + 0 = b$}
	\end{align*}
	Therefore, $w \in K \implies \{s\} + K_H \subseteq K$
\end{proof}

\subsubsection{Example}
Using the results of Exercise 2, find all solutions to the following system of linear equations:
\[
	\begin{array}{ll}
		x_1 + 2x_2 - x_3 & = 3 \\
		2x_1 + x_2 + x_3 & = 6
	\end{array}
\]

\[
	\begin{split}
		\begin{bmatrix} 1 & 2 & -1 & 3 \\ 2 & 1 & 1 & 6 \end{bmatrix} \rightarrow \begin{bmatrix} 1 & 0 & 1 & 3 \\ 0 & 1 & -1 & 0 \end{bmatrix}
		x_1 = 3 - x_3 \\
		x_2 = x_3 \\
		Solution = (3 - x_3, x_3, x_3)^T \\
		K = \{ \begin{bmatrix} 3 \\ 0 \\ 0 \end{bmatrix} + t \begin{bmatrix} -1 \\ 1 \\ 1 \end{bmatrix} | t \in F \}
	\end{split}
\]

\begin{thm}
	Let $Ax = b$ be a system of $n$ linear equations in $n$ unknowns. If $A$ is invertible, then the system has a unique solution, namely, $A^{-1}b$. Conversely, if the system has exactly one solution, then $A$ is
	invertible.
\end{thm}

\begin{proof}
	\begin{align*}
		\intertext{suppose that $A$ is invertible and substitute $A^{-1}b$ into the system $Ax = b$}
		\intertext{Then $AA^{-1}b = b \implies b = I_n b = b \implies A^{-1}b \text{ is a solution to the system}$}
		\intertext{Conversely, suppose that the system has exactly one solution. Then $Ax = b$ has a unique solution $s$.}
		\intertext{Then $s = A^{-1}b$}
		\intertext{Therefore, $A$ is invertible}
	\end{align*}
\end{proof}

\subsubsection{Example}
For each of the following systems of linear equations with invertible coefficient matrices, $A$,
\begin{itemize}
	\item Compute $A^{-1}$.
	\item Use $A^{-1}$ to find the unique solution to the system.
\end{itemize}
\begin{enumerate}
	\item $\begin{array}{ll}
			      x_1 + 3x_2  & = 2 \\
			      2x_1 + 5x_2 & = 3
		      \end{array}$
	\item $\begin{array}{ll}
			      x_1 + 2x_2 - x_3  & = 5 \\
			      x_1 + x_2 + x_3   & = 1 \\
			      2x_1 - 2x_2 + x_3 & = 4
		      \end{array}$
\end{enumerate}

\[
	\begin{split}
		A = \begin{bmatrix} 1 & 3 \\ 2 & 5 \end{bmatrix} \quad , \quad A^{-1} = -\frac{1}{5} \begin{bmatrix} 1 & -3 \\ -2 & 1 \end{bmatrix} \\
		\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = A^{-1} \begin{bmatrix} 2 \\ 3 \end{bmatrix} \\
		\begin{bmatrix} 1 & -3 \\ -2 & 1 \end{bmatrix} \begin{bmatrix} 4 \\ 3 \end{bmatrix} = \begin{bmatrix} -1 \\ 1 \end{bmatrix}
	\end{split}
\]

\[
	\begin{split}
		B = \begin{bmatrix} 1 & 2 & -1 \\ 1 & 1 & 1 \\ 2 & -2 & 1 \end{bmatrix} \quad , \quad B^{-1} = \begin{bmatrix}
			\frac{2}{3}  & 0           & \frac{1}{3}  \\
			\frac{1}{9}  & \frac{1}{3} & -\frac{2}{9} \\
			-\frac{4}{9} & \frac{2}{3} & -\frac{1}{9}
		\end{bmatrix} \\
		\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = B^{-1} \begin{bmatrix} 5 \\ 1 \\ 4 \end{bmatrix} \\
		\begin{bmatrix} \frac{2}{3}  & 0           & \frac{1}{3}  \\
                \frac{1}{9}  & \frac{1}{3} & -\frac{2}{9} \\
                -\frac{4}{9} & \frac{2}{3} & -\frac{1}{9}\end{bmatrix} \begin{bmatrix} 5 \\ 1 \\ 4 \end{bmatrix} = \begin{bmatrix} 3 \\ 0 \\ -2 \end{bmatrix}
	\end{split}
\]

This citerion involves the rank of the coefficient matrix of the system $Ax = b$ and the rank of the matrix $(A|b)$. The matrix $(A|b)$ is called the augmented matrix of the system $Ax = b$.

\begin{thm}
	Let $Ax = b$ be a system of $m$ linear equations. Then the syste mis consistent if and only if $rank(A) = rank(A|b)$.
\end{thm}

\begin{proof}
	\begin{align*}
		\intertext{Let say that the system $Ax = b$ has a solution equvalent that $b \in R(L_A)$}
		\intertext{Then $R(L_A) = span\{b\} = R(L_{(A|b)})$}
		\intertext{Then $Ax = b$ has a soution iff $b \in span\{a_1, a_2, \ldots, a_n\}$}
		\intertext{iff $span\{a_1, a_2, \ldots, a_n, b\} = span\{a_1, a_2, \ldots, a_n\}$}
		\intertext{iff $rank(A|b) = rank(A)$}
	\end{align*}
\end{proof}

\subsubsection{Example}
Determine which of the following systems of linear equations has a solution
\[
	\begin{array}{ll}
		x_1 + x_2 - x_3 + 2x_4  & = 2 \\
		x_1 + x_2 + 2x_3 \quad  & = 1 \\
		2x_1 + x_2 + x_3 + 2x_4 & = 4
	\end{array}
\]

\subsection{Aplications}
A is called a \textbf{input-output (or consumprion) matrix} and $Ap = p$ is called the equlibrium condition. For vectors $b = (b_1, b_2, \ldots, b_n)$ and $c = (c_1, c_2, \ldots, c_n)$ in $R^n$, we
use the notion $b \leq c$ to mean that $b_i \leq c_i$ for all $i$. The vector $b$ is called \textbf{nonnegative [popsitive]} if $b_i \geq 0$ [$b_i > 0$].

\begin{thm}
	Let $A$ be an $n \times n$ input-output matrix having the form
	\[
		A = \begin{bmatrix}
			B & C \\
			D & E
		\end{bmatrix}
	\]
	Where  $D$ is a $1 \times (n - 1)$ positive vector and $C$ is an $(n - 1) \times 1$ positive vecvtor. Then $(I - A)x = O$ has a one-dimensional solution set that is generated by a nonnegative vector.
\end{thm}

Ingeneral we must find a nonnegative solution to $(I - A)x = d$, where $A$ is a matrix with nonnegative entries such that the sum of the entries of each column of $A$ does not exceed one, and $d \geq 0$.It is easy to
see that if $(I - A)^{-1}$ exists and is nonnegative, then the desired solution is $(I - A)^{-1}d$.


\section{Systems of Linear Equations - Computational Aspects}

In section 3.3, we obtained a necessary and sufficient condition for a system of linear equations to have solutions and learned how to express the solutions to a nonhomogenous system in terms of solutions
to the corresponding homogenous system. The latter result enables us to determine all the solutions to a give nsystem if we can find one solution to the given system and a basis for the solution set
of the corresponding homogenous system. In this section, we use elementary row operations to accomplish these two objective simultaneously. The essence of this technique is to transfomr a give nssytem
of linear equations into a system having the same solution, but whic his easier to solve

\begin{defn}
	Two systems of linear equations are called \textbf{equivalent} if they have the same solution set.
\end{defn}

\begin{thm}
	Let $Ax = b$ be a system of $b$ linear equations in $n$ unknowns, and let $C$ be an invertible $m \times m$ matrix. Then the system $(CA)x = Cb$ is equivalent to $Ax = b$.
\end{thm}

\begin{proof}
	\begin{align*}
		d
	\end{align*}
\end{proof}

\begin{cor}
	Let $Ax = b$ be a system of $m$ linear equations in $n$ unknowns. If $(A'|b')$ is obtained from $(A|b)$ by a finite number of elementary row operations, then the system $A'x = b'$ is equivalent to $Ax = b$.
\end{cor}

by using elementary row operations, we transform the augmented matrix into an upper triangular matrix in which the first nonzero entry of each row is $1$, and it occurs in a column to the right of the first
nonzerto entry of eac hproceding row. (Recall that amatrix $A$ is upper triangular if $A_{ij} = 0$ whenever $i > j$.)

\subsubsection{Example}
Use elementary row operations to transform each of the following systems of linear equations into an equivalent system in triangular form, and then solve the system:

\[
	\begin{array}{ll}
		x_1 + 2x_2 - x_3   & = -1 \\
		2x_1 + 2x_2 + x_3  & = 1  \\
		3x_1 + 5x_2 - 2x_3 & = 2
	\end{array}
\]

\[
	\begin{split}
		\begin{bmatrix} 1 & 2 & -1 & -1 \\ 2 & 2 & 1 & 1 \\ 3 & 5 & -2 & 2 \end{bmatrix} \rightarrow \begin{bmatrix} 1 & 2 & -1 & -1 \\ 0 & -2 & 3 & 3 \\ 0 & -1 & 1 & 2 \end{bmatrix} \\
	\end{split}
\]

\begin{defn}
	A matrix is said to be in \textbf{reduced row echelon form} if the following three conditions are satisfied.

	\begin{enumerate}
		\item any row contaioning a nonzero entry precedes any row in whic all the entriesx are zxero (if any).
		\item The first nonzero entry in each row is the onl;y nonzero entry in tits column
		\item The firs nonzero entry in each row is $1$ and it occurs un a column to the right of the firs nonzero entry in the preceding row.
	\end{enumerate}
\end{defn}

The procedure described previously for reducing an augmented matrix is called \textbf{Gaussiaan elimination} it consits of two seperates parts:

\begin{enumerate}
	\item In the foward pass, (septsx 1-5) the augmented amtrix is transformed into a nupper triangular matrix in whic hthe first nonxero entry of eac hrow is 1, and it occurs in a column to the right of
	      the first nonzero entry of each row.
	\item In the backward pass or back-subsitution (steps 6-7), the upper triangualr matrix is transformed into reduced row echolon form by making the first nonzero entry of each row the only nonzer
	      entry of its column
\end{enumerate}

\begin{thm}
	Gaussian elimation transformas any matrix into its reduced row echelon form.p
\end{thm}

\[
	\begin{array}{ll}
		x_1 + 2x_2 - x_3 + 3x_4                   & = 2 \\
		2x_1 + 4x_1 - x_3 + 6x_4                  & = 5 \\
		\quad \quad \quad x_2 \quad \quad  + 2x_4 & = 3
	\end{array}
\]

\[
	\begin{array}{ll}
		x_1 + x_2  -3x_2 + x_4            & = 2 \\
		x_1 + x_2 + x_3 - x_4             & = 2 \\
		x_1 + x_2 - x_3 \quad \quad \quad & = 0
	\end{array}
\]

\begin{thm}
	Let $Ax = b$ be a system of $r$ nonzero equations in $n$ unknowns. Suppose that $rank(A) = rank(A|b)$ and that $(A|b)$ is in reduced row echelon form. Then
	\begin{enumerate}
		\item $rank(A) = r$
		\item If the general solution obtrained by the procedure above is of the form
		      \[
			      s = s_0 + t_1 u_1 + t_2 u_2 + \ldots + t_{n-r} u_{n-r}
		      \]
	\end{enumerate}
	the $\{u_1, u_2, \ldots, u_{n-r}\}$ is a basis for the solution set of the corresponding homogeneous system, and $s_0$ is a solution to the original system.
\end{thm}

\subsection{An interpretation of the reduced row echelon form}

Let $A$ be an $m \times n$ matrix with columns $a_1, a_2, \ldots, a_n$, and let $B$ be the reduced row echelon form of $A$. Denote the columns of $B$ by $b_1, b_2, \ldots, b_n$. If the rank of $A$ is
$r$, then the rank of $B$ is also $r$ by the corollary to Theorem 3.4. Because $B$ is in reduced row echelon form, no nonzero row of $B$ can be a linear combination of the other rows of $B$. Hence $B$
must have exactly $r$ nonzero rows, and if $r \leq 1$, the vectors $e_1, e_2, \ldots, e_r$ must occure among the columns of $B$. For $i = 1, 2, \ldots, r$, let $j_i$ denote a collumn number of $B$ such
that $b_{j_i} = e_i$. We claim that $\{a_{j_1}, a_{j_2}, \ldots, a_{j_r}\}$, the columns of $A$ corresponding to these columns of $B$, are linearly independent. For suppose that there are scalars $c_1, c_2, \ldots, c_r$
such that $c_1a_{j_1} + c_2a_{j_2} + \ldots + c_ra_{j_r} = 0$.

\begin{thm}
	Let $A$ be an $m \times n$ matrix of rank $r$. Where $r \geq 0$, and let $B$ be the reduce row echelon form of $A$. Then
	\begin{enumerate}
		\item The number of nonzero row in $B$ is $r$.
		\item For each $i = 1, 2, \ldots, r$, there is a column $b_{j_i}$ of $B$ such that $b_{j_i} = e_i$.
		\item The columns of $A$ numbered $j_1, j_2, \ldots, j_r$ are linearly independent.
		\item for each $k = 1, 2, \ldots, n$, if column $k$ of $B$ is $d_1e_1 + d_2e_2 + \ldots + d_re_r$, then culumn $k$ of $A$ is $d_1a_{j_1} + d_2a_{j_2} + \ldots + d_ra_{j_r}$.
	\end{enumerate}
\end{thm}

\subsubsection{Example}
Let
\[
	V = \{ (x_1, x_2, x_3, x_4, x_5) \in \R^5 | x_1 - 2x_2 + 3x_3 - x_4 + 2x_5 = 0 \}
\]
\begin{enumerate}
	\item Show that $S = \{ (0,1,1,1,0) \}$ is a linearly independent subset of $V$.
	\item Extend $S$ to a basis for $V$.
\end{enumerate}

\end{document}
